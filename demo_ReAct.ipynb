{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f3997-3802-4289-a17b-f36d0d9c4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from server.gptq.gptq import *\n",
    "from server.gptq.modelutils import *\n",
    "from server.gptq.quant import *\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import guidance\n",
    "\n",
    "DEV = torch.device('cuda:0')\n",
    "\n",
    "def get_llama(model):\n",
    "    import torch\n",
    "    def skip(*args, **kwargs):\n",
    "        pass\n",
    "    torch.nn.init.kaiming_uniform_ = skip\n",
    "    torch.nn.init.uniform_ = skip\n",
    "    torch.nn.init.normal_ = skip\n",
    "    from transformers import LlamaForCausalLM\n",
    "    model = LlamaForCausalLM.from_pretrained(model, torch_dtype='auto')\n",
    "    model.seqlen = 2048\n",
    "    return model\n",
    "\n",
    "def load_quant(model, checkpoint, wbits, groupsize):\n",
    "    from transformers import LlamaConfig, LlamaForCausalLM \n",
    "    config = LlamaConfig.from_pretrained(model)\n",
    "    def noop(*args, **kwargs):\n",
    "        pass\n",
    "    torch.nn.init.kaiming_uniform_ = noop \n",
    "    torch.nn.init.uniform_ = noop \n",
    "    torch.nn.init.normal_ = noop \n",
    "\n",
    "    torch.set_default_dtype(torch.half)\n",
    "    transformers.modeling_utils._init_weights = False\n",
    "    torch.set_default_dtype(torch.half)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    torch.set_default_dtype(torch.float)\n",
    "    model = model.eval()\n",
    "    layers = find_layers(model)\n",
    "    for name in ['lm_head']:\n",
    "        if name in layers:\n",
    "            del layers[name]\n",
    "    make_quant(model, layers, wbits, groupsize)\n",
    "\n",
    "    print('Loading model ...')\n",
    "    if checkpoint.endswith('.safetensors'):\n",
    "        from safetensors.torch import load_file as safe_load\n",
    "        model.load_state_dict(safe_load(checkpoint))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "    model.seqlen = 2048\n",
    "    print('Done.')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823bb6e4-5f01-4ef2-a794-6e997cbfd8f1",
   "metadata": {},
   "source": [
    "## Define a tool\n",
    "We define a search tool with GoogleSerperAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b92049-03e4-466c-824a-f46de7490a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "os.environ[\"SERPER_API_KEY\"] = 'YOUR_API_KEY'\n",
    "search = GoogleSerperAPIWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855610f-6334-4ad8-a559-f980697968b4",
   "metadata": {},
   "source": [
    "## Load model with GPTQ\n",
    "Please download the model from Huggingface and change these paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370ff5e-fb65-4c3a-8141-4a685812aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_para = '/home/quang/working/LLMs/oobabooga_linux/text-generation-webui/models/TheBloke_wizard-mega-13B-GPTQ'\n",
    "checkpoint_para = '/home/quang/working/LLMs/oobabooga_linux/text-generation-webui/models/TheBloke_wizard-mega-13B-GPTQ/wizard-mega-13B-GPTQ-4bit-128g.no-act.order.safetensors'\n",
    "model = load_quant(model_para, checkpoint_para, 4, 128)\n",
    "model.to(DEV)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff593e-da83-44a4-82a3-f4634ab74444",
   "metadata": {},
   "source": [
    "## Set Guidance LLM as our local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd035b7-c01a-47dc-969c-7433b2b55f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = guidance.llms.Transformers(model=model, tokenizer=tokenizer, device=0)\n",
    "guidance.llm = llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed0f8b",
   "metadata": {},
   "source": [
    "## LOAD GUIDANCE WITH llama_quantized.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa22145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "from llama_quantized import LLaMAQuantized\n",
    "from search import web_search, find_closest_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b0d1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following quantized model: /home/pwalch/projects/textgen/models/4bit_WizardLM-13B-Uncensored-4bit-128g/4bit-128g.safetensors\n",
      "Model to device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: /home/pwalch/projects/textgen/models//4bit_WizardLM-13B-Uncensored-4bit-128g/\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the MODEL_PATH and MODEL_NAME from environment variables\n",
    "model_dir = os.getenv(\"MODEL_PATH\")\n",
    "model = os.getenv(\"MODEL_NAME\")\n",
    "\n",
    "# Initialize the LLaMAQuantized model\n",
    "#llama_model = LLaMAQuantized(model_dir=model_dir, model=model)\n",
    "guidance.llm = LLaMAQuantized(model_dir=model_dir, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116eab4f-f7b5-4428-9827-7600bd4d4cdd",
   "metadata": {},
   "source": [
    "## Let's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef58441-84d6-4df6-ac60-7ccfebd3c60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-0ca7a888-3b46-49eb-86c1-a5c585a1213a\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-0ca7a888-3b46-49eb-86c1-a5c585a1213a\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
       "\n",
       "### Instruction:\n",
       "Answer the following questions as best you can. You have access to the following tools:\n",
       "\n",
       "Google Search: A wrapper around Google Search. Useful for when you need to answer questions about current events. The input is the question to search relavant information.\n",
       "\n",
       "Strictly use the following format:\n",
       "\n",
       "Question: the input question you must answer\n",
       "Thought: you should always think about what to do\n",
       "Action: the action to take, should be one of [Google Search]\n",
       "Action Input: the input to the action, should be a question.\n",
       "Observation: the result of the action\n",
       "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
       "Thought: I now know the final answer\n",
       "Final Answer: the final answer to the original input question\n",
       "\n",
       "For examples:\n",
       "Question: How old is CEO of Microsoft wife?\n",
       "Thought: First, I need to find who is the CEO of Microsoft.\n",
       "Action: Google Search\n",
       "Action Input: Who is the CEO of Microsoft?\n",
       "Observation: Satya Nadella is the CEO of Microsoft.\n",
       "Thought: Now, I should find out Satya Nadella&#x27;s wife.\n",
       "Action: Google Search\n",
       "Action Input: Who is Satya Nadella&#x27;s wife?\n",
       "Observation: Satya Nadella&#x27;s wife&#x27;s name is Anupama Nadella.\n",
       "Thought: Then, I need to check Anupama Nadella&#x27;s age.\n",
       "Action: Google Search\n",
       "Action Input: How old is Anupama Nadella?\n",
       "Observation: Anupama Nadella&#x27;s age is 50.\n",
       "Thought: I now know the final answer.\n",
       "Final Answer: Anupama Nadella is 50 years old.\n",
       "\n",
       "### Input:\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{question}}</span>\n",
       "\n",
       "### Response:\n",
       "Question: <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{question}}</span>\n",
       "Thought: <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &#x27;thought&#x27; stop=&#x27;\\n&#x27;}}</span>\n",
       "Action: <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{select &#x27;tool_name&#x27; options=valid_tools}}</span>\n",
       "Action Input: <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &#x27;actInput&#x27; stop=&#x27;\\n&#x27;}}</span>\n",
       "Observation:<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{search actInput}}</span>\n",
       "Thought: <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &#x27;thought2&#x27; stop=&#x27;\\n&#x27;}}</span>\n",
       "Final Answer: <span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &#x27;final&#x27; stop=&#x27;\\n&#x27;}}</span></pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"0ca7a888-3b46-49eb-86c1-a5c585a1213a\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m result \u001b[39m=\u001b[39m prompt(question\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIs Eminem a football player?\u001b[39m\u001b[39m'\u001b[39m, search\u001b[39m=\u001b[39msearchGoogle, valid_answers\u001b[39m=\u001b[39mvalid_answers, valid_tools\u001b[39m=\u001b[39mvalid_tools)\n\u001b[1;32m     57\u001b[0m \u001b[39m# Flush the output to the console immediately.\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m     60\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = ['Google Search']\n",
    "\n",
    "prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "Google Search: A wrapper around Google Search. Useful for when you need to answer questions about current events. The input is the question to search relavant information.\n",
    "\n",
    "Strictly use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [Google Search]\n",
    "Action Input: the input to the action, should be a question.\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "For examples:\n",
    "Question: How old is CEO of Microsoft wife?\n",
    "Thought: First, I need to find who is the CEO of Microsoft.\n",
    "Action: Google Search\n",
    "Action Input: Who is the CEO of Microsoft?\n",
    "Observation: Satya Nadella is the CEO of Microsoft.\n",
    "Thought: Now, I should find out Satya Nadella's wife.\n",
    "Action: Google Search\n",
    "Action Input: Who is Satya Nadella's wife?\n",
    "Observation: Satya Nadella's wife's name is Anupama Nadella.\n",
    "Thought: Then, I need to check Anupama Nadella's age.\n",
    "Action: Google Search\n",
    "Action Input: How old is Anupama Nadella?\n",
    "Observation: Anupama Nadella's age is 50.\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: Anupama Nadella is 50 years old.\n",
    "\n",
    "### Input:\n",
    "{{question}}\n",
    "\n",
    "### Response:\n",
    "Question: {{question}}\n",
    "Thought: {{gen 'thought' stop='\\\\n'}}\n",
    "Action: {{select 'tool_name' options=valid_tools}}\n",
    "Action Input: {{gen 'actInput' stop='\\\\n'}}\n",
    "Observation:{{search actInput}}\n",
    "Thought: {{gen 'thought2' stop='\\\\n'}}\n",
    "Final Answer: {{gen 'final' stop='\\\\n'}}\"\"\"\n",
    "\n",
    "def searchGoogle(t):    \n",
    "    return search.run(t)\n",
    "\n",
    "prompt = guidance(prompt_template)\n",
    "result = prompt(question='Is Eminem a football player?', search=searchGoogle, valid_answers=valid_answers, valid_tools=valid_tools)\n",
    "\n",
    "# Flush the output to the console immediately.\n",
    "sys.stdout.flush()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812ee13-8252-4488-bf0f-51872d28c66e",
   "metadata": {},
   "source": [
    "# Build the agent with Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213706a-9b64-4d8b-b769-13b163367ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance.llm.cache.clear()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d69e8-78a8-499c-a34c-1c98eea2b23a",
   "metadata": {},
   "source": [
    "### Define prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea7e11-244b-4aad-8f54-688b5f827920",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = ['Google Search']\n",
    "\n",
    "prompt_start_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "Google Search: A wrapper around Google Search. Useful for when you need to answer questions about current events. The input is the question to search relavant information.\n",
    "\n",
    "Strictly use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [Google Search]\n",
    "Action Input: the input to the action, should be a question.\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "For examples:\n",
    "Question: How old is CEO of Microsoft wife?\n",
    "Thought: First, I need to find who is the CEO of Microsoft.\n",
    "Action: Google Search\n",
    "Action Input: Who is the CEO of Microsoft?\n",
    "Observation: Satya Nadella is the CEO of Microsoft.\n",
    "Thought: Now, I should find out Satya Nadella's wife.\n",
    "Action: Google Search\n",
    "Action Input: Who is Satya Nadella's wife?\n",
    "Observation: Satya Nadella's wife's name is Anupama Nadella.\n",
    "Thought: Then, I need to check Anupama Nadella's age.\n",
    "Action: Google Search\n",
    "Action Input: How old is Anupama Nadella?\n",
    "Observation: Anupama Nadella's age is 50.\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: Anupama Nadella is 50 years old.\n",
    "\n",
    "### Input:\n",
    "{{question}}\n",
    "\n",
    "### Response:\n",
    "Question: {{question}}\n",
    "Thought: {{gen 't1' stop='\\\\n'}}\n",
    "{{select 'answer' options=valid_answers}}: \"\"\"\n",
    "\n",
    "prompt_mid_template = \"\"\"{{history}}{{select 'tool_name' options=valid_tools}}\n",
    "Action Input: {{gen 'actInput' stop='\\\\n'}}\n",
    "Observation: {{do_tool tool_name actInput}}\n",
    "Thought: {{gen 'thought' stop='\\\\n'}}\n",
    "{{select 'answer' options=valid_answers}}: \"\"\"\n",
    "\n",
    "prompt_final_template = \"\"\"{{history}}{{select 'tool_name' options=valid_tools}}\n",
    "Action Input: {{gen 'actInput' stop='\\\\n'}}\n",
    "Observation: {{do_tool tool_name actInput}}\n",
    "Thought: {{gen 'thought' stop='\\\\n'}}\n",
    "{{select 'answer' options=valid_answers}}: {{gen 'fn' stop='\\\\n'}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6ad72-f7a4-444f-a816-712aae88d65f",
   "metadata": {},
   "source": [
    "### Define tools, feel free to add more tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab90dcf-2c48-4073-bcef-54c7f5f36d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchGoogle(key_word):    \n",
    "    return search.run(key_word)\n",
    "\n",
    "dict_tools = {\n",
    "    'Google Search': searchGoogle\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b8576-0d21-4495-93c7-a3a67451782c",
   "metadata": {},
   "source": [
    "### Our agent with Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f9585-b42a-455f-a504-1b16ca6ed444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAgentGuidance:\n",
    "    def __init__(self, guidance, tools, num_iter=3):\n",
    "        self.guidance = guidance\n",
    "        self.tools = tools\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "    def do_tool(self, tool_name, actInput):\n",
    "        return self.tools[tool_name](actInput)\n",
    "    \n",
    "    def __call__(self, query):\n",
    "        prompt_start = self.guidance(prompt_start_template)\n",
    "        result_start = prompt_start(question=query, valid_answers=valid_answers)\n",
    "\n",
    "        result_mid = result_start\n",
    "        \n",
    "        for _ in range(self.num_iter - 1):\n",
    "            if result_mid['answer'] == 'Final Answer':\n",
    "                break\n",
    "            history = result_mid.__str__()\n",
    "            prompt_mid = self.guidance(prompt_mid_template)\n",
    "            result_mid = prompt_mid(history=history, do_tool=self.do_tool, valid_answers=valid_answers, valid_tools=valid_tools)\n",
    "        \n",
    "        if result_mid['answer'] != 'Final Answer':\n",
    "            history = result_mid.__str__()\n",
    "            prompt_mid = self.guidance(prompt_final_template)\n",
    "            result_final = prompt_mid(history=history, do_tool=self.do_tool, valid_answers=['Final Answer'], valid_tools=valid_tools)\n",
    "        else:\n",
    "            history = result_mid.__str__()\n",
    "            prompt_mid = self.guidance(history + \"{{gen 'fn' stop='\\\\n'}}\")\n",
    "            result_final = prompt_mid()\n",
    "        return result_final['fn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63155038-662c-4795-a66f-21f1278f46a0",
   "metadata": {},
   "source": [
    "### Okay, let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf966ea-1398-4bd0-821b-a62142bbf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_agent = CustomAgentGuidance(guidance, dict_tools)\n",
    "\n",
    "list_queries = [\n",
    "    \"How much is the salary of number 8 of Manchester United?\",\n",
    "    \"What is the population of Congo?\",\n",
    "    \"Where was the first president of South Korean born?\",\n",
    "    \"What is the population of the country that won World Cup 2022?\"    \n",
    "]\n",
    "\n",
    "final_answer = custom_agent(list_queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941cd3fb-0b56-4a20-b0b9-f883c112cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = custom_agent(list_queries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163881c-23a1-4a09-8e4d-34d849fb1ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
